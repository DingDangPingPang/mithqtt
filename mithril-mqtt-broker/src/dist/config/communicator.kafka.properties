# Kafka communicator configuration

# Communicator

# Communicator implementation (full qualified class name)
communicator.class = com.github.longkerdandy.mithril.mqtt.communicator.kafka.broker.KafkaBrokerCommunicator

# This is the topic prefix that broker instance consume. (full topic is like mithril.mqtt.broker.{brokerId})
# You should create this Kafka topics before starting each broker.
# In Kafka there cannot be more consumer instances than partitions.
# eg. Each broker instance running 4 consumer threads, which means there should be at least 4 partitions.
communicator.broker.topic = mithril.mqtt.broker

# This is the topic that all processor instance consume with the same consumer group. (to achieve load balance)
# You should create this Kafka topic before starting processor.
# In Kafka there cannot be more consumer instances than partitions.
# eg. There are 4 processor instances, each running 16 consumer threads, which means there should be at least 64 partitions.
communicator.processor.topic = mithril.mqtt.processor

# This is the topic that processor will pass message to 3rd party application
# You should create this Kafka topic before starting processor & your application.
communicator.application.topic = mithril.mqtt.application

# Kafka Producer

# A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. Data will be load
# balanced over all servers irrespective of which servers are specified here for bootstrapping&mdash;this list only
# impacts the initial hosts used to discover the full set of servers. This list should be in the form
# <code>host1:port1,host2:port2,...</code>. Since these servers are just used for the initial connection to
# discover the full cluster membership (which may change dynamically), this list need not contain the full set of
# servers (you may want more than one, though, in case a server is down). If no server in this list is available sending
# data will fail until on becomes available.
bootstrap.servers = localhost:9092

# The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the
# durability of records that are sent. The following settings are common:
# <ul>
# <li><code>acks=0</code> If set to zero then the producer will not wait for any acknowledgment from the
# server at all. The record will be immediately added to the socket buffer and considered sent. No guarantee can be
# made that the server has received the record in this case, and the <code>retries</code> configuration will not
# take effect (as the client won't generally know of any failures). The offset given back for each record will
# always be set to -1.
# <li><code>acks=1</code> This will mean the leader will write the record to its local log but will respond
# without awaiting full acknowledgement from all followers. In this case should the leader fail immediately after
# acknowledging the record but before the followers have replicated it then the record will be lost.
# <li><code>acks=all</code> This means the leader will wait for the full set of in-sync replicas to
# acknowledge the record. This guarantees that the record will not be lost as long as at least one in-sync replica
# remains alive. This is the strongest available guarantee.
acks = 1


# Kafka Consumer

# Specifies the ZooKeeper connection string in the form hostname:port where host and port are the host and port of a ZooKeeper server.
# To allow connecting through other ZooKeeper nodes when that ZooKeeper machine is down
# you can also specify multiple hosts in the form hostname1:port1,hostname2:port2,hostname3:port3.
# The server may also have a ZooKeeper chroot path as part of it's ZooKeeper connection string
# which puts its data under some path in the global ZooKeeper namespace.
# If so the consumer should use the same chroot path in its connection string.
# For example to give a chroot path of /chroot/path
# you would give the connection string as hostname1:port1,hostname2:port2,hostname3:port3/chroot/path.
zookeeper.connect = localhost:2181

# This apply to application communicator only.
# A string that uniquely identifies the group of consumer processes to which this consumer belongs.
# By setting the same group id multiple processes indicate that they are all part of the same consumer group.
group.id = mithril.mqtt.broker.consumer

# The first thing to know about using a High Level Consumer is that it can (and should!) be a multi-threaded application.
# The threading model revolves around the number of partitions in your topic and there are some very specific rules:
# if you provide more threads than there are partitions on the topic, some threads will never see a message
# if you have more partitions than you have threads, some threads will receive data from multiple partitions
# if you have multiple partitions per thread there is NO guarantee about the order you receive messages,
# other than that within the partition the offsets will be sequential.
# For example, you may receive 5 messages from partition 10 and 6 from partition 11,
# then 5 more from partition 10 followed by 5 more from partition 10 even if partition 11 has data available.
# adding more processes/threads will cause Kafka to re-balance, possibly changing the assignment of a Partition to a Thread.
consumer.threads = 4